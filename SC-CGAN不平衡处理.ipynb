{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e23e0070-6512-4290-ae86-0cd0abfcdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.init as init  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf92185c-dcd7-4851-bd5d-3e5e760360e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集  \n",
    "train_df = pd.read_csv('./数据/KDDTrain+afterP.csv')  # 使用ADASYN进行数据不平衡处理（数据增强）的效果是最好的\n",
    "test_df = pd.read_csv('./数据/KDDTest+afterP.csv')\n",
    "\n",
    "# 划分训练集和测试集的标签和特征\n",
    "y_train = train_df['attack_type']\n",
    "X_train = train_df.drop(columns = ['attack_type'])\n",
    "y_test = test_df['attack_type']\n",
    "X_test = test_df.drop(columns = ['attack_type'])\n",
    "\n",
    "# 需要把类别特征进行数值化：['dos','normal','probe','r2l','u2r']分别映射为0 1 2 3 4\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "# 初始化LabelEncoder  \n",
    "le = LabelEncoder()  \n",
    "  \n",
    "# 对y_train中的类别特征进行数值化  \n",
    "y_train_encoded = le.fit_transform(y_train)  \n",
    "\n",
    "# 对y_test中的类别特征进行数值化  \n",
    "y_test_encoded = le.fit_transform(y_test)  \n",
    "\n",
    "# 将数据转换为PyTorch张量格式,才能进行后续的运算 （需要Numpy格式进行转换）\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)  \n",
    "y_train_encoded = torch.tensor(y_train_encoded, dtype=torch.long)  \n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)  \n",
    "y_test_encoded = torch.tensor(y_test_encoded, dtype=torch.long) \n",
    "\n",
    "# 使用one_hot函数将类别数字转换为独热编码, 条件变分自编码器需要输入标签最好是进行了独热编码  \n",
    "y_train_encoded = F.one_hot(y_train_encoded, num_classes=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6216a2-653d-479d-a323-9e6d2ac199a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7bb33a3-3042-42d0-bcee-7ec9e9bd1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据加载器  \n",
    "train_dataset = TensorDataset(X_train, y_train_encoded)  \n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)  \n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test_encoded)  \n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c51eef-ace7-4d4d-b3f4-b3a336001f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 条件批量归一化：把数据的类别信息融合到归一化过程中，用类别信息控制缩放（weight）和bias\n",
    "# 公式结合了标准批归一化和类别特定的偏移和缩放，使得归一化过程能够依赖于输入的类别标签\n",
    "class ConditionalBatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(ConditionalBatchNorm, self).__init__()\n",
    "        # num_features: 批归一化层需要处理的特征数量。\n",
    "        # num_classes: 类别标签的数量。\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # gamma: 可学习的缩放因子，大小为num_features。\n",
    "        # beta: 可学习的偏移量，初始化为零，大小为num_features。\n",
    "        # weight: 类别特定的缩放因子，大小为num_classes x num_features。\n",
    "        # bias: 类别特定的偏移量，大小为num_classes x num_features。\n",
    "        # 这些都可以通过参数进行学习\n",
    "        self.gamma = nn.Parameter(torch.Tensor(num_features))\n",
    "        #######使用标准正态分布初始化缩放因子，防止输出过大或者过小\n",
    "        nn.init.normal_(self.gamma, mean=0, std=1)  \n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))  \n",
    "        self.weight = nn.Parameter(torch.Tensor(num_classes, num_features))  # 这些全都是可以训练的参数 一个表格存储各个类的w和b\n",
    "        self.bias = nn.Parameter(torch.zeros(num_classes, num_features)) \n",
    "        ####### 使用 Xavier Uniform 初始化，防止初始化值异常导致输出异常，出现损失函数异常\n",
    "        nn.init.xavier_uniform_(self.weight)  \n",
    "  \n",
    "    def forward(self, x, y):  \n",
    "        batch_size = x.size(0)  \n",
    "        # 通过y索引相应类别的w和b；重塑w和b与x相同形状\n",
    "        # 使用矩阵乘法实现通过独热编码y选择\n",
    "        y = y.float()\n",
    "        weight = torch.matmul(y, self.weight) \n",
    "        bias = torch.matmul(y, self.bias) \n",
    "        # 对x的最后一个特征维度做归一化（减去均值除以方差）\n",
    "        x_mean = x.mean(-1, keepdim=True)           #.mean(-2, keepdim=True)  \n",
    "        x_var = x.var(-1, keepdim=True)             # .var(-2, keepdim=True)  \n",
    "        return self.gamma * (x - x_mean) / torch.sqrt(x_var + 1e-5) + self.beta + weight * x_mean + bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0dcb3c6-5293-498a-988f-de75cb6302d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 条件自注意力机制：更关注条件相关的特征（乘更大的权重），从而生成更符合条件（类别）的特征\n",
    "# 采用自注意力机制！######### 当前元素与其他元素的相似度计算注意力权重\n",
    "# 捕捉不同特征之间的远距离依赖关系\n",
    "class ConditionalSelfAttention(nn.Module):\n",
    "    def __init__(self, in_features, label_dim, attention_dim):\n",
    "        super(ConditionalSelfAttention, self).__init__()\n",
    "        # QKV就是三个输入X的线性变换，通过三个可以训练的权重增强模型的拟合能力；\n",
    "        self.query = nn.Linear(in_features, attention_dim, bias=False)\n",
    "        self.key = nn.Linear(in_features, attention_dim, bias=False)\n",
    "        self.value = nn.Linear(in_features, attention_dim, bias=False)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1)) # 可学习的参数 控制注意力机制的缩放\n",
    "        # 将输入编码为稠密向量，并且变成注意力维度，与QKV逐元素合并，引入条件因素;embedding接受离散的索引作为输入（和Linear类似）\n",
    "        self.label_embedding = nn.Linear(label_dim, attention_dim)\n",
    "        self.fc = nn.Linear(attention_dim,in_features)\n",
    "        # 使用xaiver初始化\n",
    "        init.xavier_uniform_(self.query.weight)\n",
    "        init.xavier_uniform_(self.key.weight)\n",
    "        init.xavier_uniform_(self.value.weight)\n",
    "        init.xavier_uniform_(self.label_embedding.weight)\n",
    "        init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # 条件嵌入\n",
    "        labels = labels.to(torch.float32)\n",
    "        label_emb = self.label_embedding(labels)\n",
    "        # print(label_emb.shape) # 128 30\n",
    "        # 计算 QKV （x的线性变换）\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        # print(q.shape) # 128 30\n",
    "        # 加入条件信息QKV\n",
    "        q = q + label_emb  \n",
    "        k = k + label_emb  \n",
    "        v = v + label_emb \n",
    "        q = q.unsqueeze(2)# 列向量\n",
    "        k = k.unsqueeze(2)# 列向量 下面转置变成行向量\n",
    "        # print(q.shape) # 128 30 1\n",
    "        # 计算注意力权重（核心公式）\n",
    "        # QK转置 做batch矩阵乘法得到注意力分数（特征之间相关程度、关联性） 列向量乘行向量得到矩阵\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        # print(attn.shape) # 128 30 30\n",
    "        # 除以缩放因子:缩放因子是根号下k的维度 防止分数过大 梯度饱和 type as保证相同的数据类型和设备类型\n",
    "        attn = attn / torch.sqrt(torch.tensor(k.size(-1)).type_as(q))  \n",
    "        # 归一化 将每一列上的注意力分数的和设为一\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        v = v.unsqueeze(2) # 128 30 1\n",
    "        # 应用注意力权重到 V（加权求和） 30 30 x 30 1 = 30 1\n",
    "        attn_output = torch.bmm(attn, v)\n",
    "        # print(attn_output.shape)\n",
    "        attn_output = attn_output.squeeze(-1)\n",
    "        attn_output = self.fc(attn_output)\n",
    "        # 残差连接\n",
    "        attn_output = self.gamma * attn_output + x\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10417be-dce4-46ca-9dd4-e78544d36e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):  \n",
    "    def __init__(self, input_dim, hidden_dim):  \n",
    "        super(CNN, self).__init__()  \n",
    "        self.fc_r = nn.Linear(input_dim,hidden_dim)\n",
    "        init.xavier_uniform_(self.fc_r.weight)  # 使用Xavier均匀分布初始化权重 \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)  \n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln1 = nn.LayerNorm(normalized_shape=[60])\n",
    "        self.ln2 = nn.LayerNorm(normalized_shape=[30])  \n",
    "        self.leakyrelu = nn.LeakyReLU()  \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)  \n",
    "        self.flatten = nn.Flatten()  \n",
    "        self.fc = nn.Linear(128 * 15, 1)\n",
    "        self.sigmoid = nn.Sigmoid()  \n",
    "        init.xavier_uniform_(self.conv1.weight)\n",
    "        init.xavier_uniform_(self.conv2.weight)\n",
    "        init.xavier_uniform_(self.conv3.weight)\n",
    "        init.xavier_uniform_(self.conv4.weight)\n",
    "        init.xavier_uniform_(self.fc.weight)\n",
    "  \n",
    "    def forward(self, x): \n",
    "        x = self.fc_r(x)\n",
    "        x = x.unsqueeze(1)  # 增加通道维度  b 1 60\n",
    "        x = self.conv1(x)  # b 16 60\n",
    "        x = self.ln1(x)\n",
    "        x = self.leakyrelu(x) \n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)  # b 32 60\n",
    "        x = self.ln1(x)\n",
    "        x = self.leakyrelu(x) \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.maxpool(x) \n",
    "        \n",
    "        x = self.conv3(x)  # b 64 30\n",
    "        x = self.ln2(x)\n",
    "        x = self.leakyrelu(x) \n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x)  # b 128 30\n",
    "        x = self.ln2(x)\n",
    "        x = self.leakyrelu(x) \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.flatten(x)  # b 128x15\n",
    "        x = self.fc(x)  # b 1\n",
    "        x = self.sigmoid(x) # 输出一个概率值，是真数据还是假数据 表示输入数据是真实的概率\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7537cc16-bd79-460c-8de3-7763854e6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义GAN模型\n",
    "\n",
    "# 条件GAN：定义，需要输入标签，条件信息就是标签，与噪声结合后生成数据\n",
    "# 生成器：输入噪声 输出生成的数据；\n",
    "class Generator(nn.Module):  \n",
    "    def __init__(self, input_dim, label_dim, output_dim):  \n",
    "        super(Generator, self).__init__()  \n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim + label_dim, 100) \n",
    "        \n",
    "        self.fc21 = nn.Linear(100,100)\n",
    "        self.fc22 = nn.Linear(100,100)\n",
    "        self.fc23 = nn.Linear(100,100)\n",
    "        self.fc24 = nn.Linear(100,100)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.bn3 = nn.BatchNorm1d(100)\n",
    "        self.bn4 = nn.BatchNorm1d(100)\n",
    "        self.bn5 = nn.BatchNorm1d(100)\n",
    "        \n",
    "        self.bn_output = nn.BatchNorm1d(output_dim)\n",
    "        self.fc3 = nn.Linear(100, output_dim)\n",
    "        self.csa1 = ConditionalSelfAttention(100,label_dim,30)\n",
    "        self.csa2 = ConditionalSelfAttention(100,label_dim,30)\n",
    "        # 使用的是RELU的变体 用Kaiming初始化\n",
    "        init.kaiming_normal_(self.fc1.weight, mode='fan_out', nonlinearity='relu')  \n",
    "        init.kaiming_normal_(self.fc21.weight, mode='fan_out', nonlinearity='relu') \n",
    "        init.kaiming_normal_(self.fc22.weight, mode='fan_out', nonlinearity='relu') \n",
    "        init.kaiming_normal_(self.fc23.weight, mode='fan_out', nonlinearity='relu') \n",
    "        init.kaiming_normal_(self.fc24.weight, mode='fan_out', nonlinearity='relu') \n",
    "        init.kaiming_normal_(self.fc3.weight, mode='fan_out', nonlinearity='relu')   \n",
    "        \n",
    "        \n",
    "   # 输入随机噪声\n",
    "    def forward(self, z, labels):  \n",
    "        combined = torch.cat((z, labels), 1)  # 将标签和噪声拼接\n",
    "        x = self.fc1(combined)  \n",
    "        x = self.bn1(x)  \n",
    "        x = F.leaky_relu(x) \n",
    "        x = self.fc21(x)\n",
    "        x = self.csa1(x, labels) # 条件自注意力机制\n",
    "        x = self.bn2(x)        \n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc22(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc23(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc24(x)\n",
    "        # x = self.csa2(x, labels) # 条件自注意力机制\n",
    "        x = self.bn5(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn_output(x)\n",
    "        return x  \n",
    "    \n",
    "class Discriminator(nn.Module):  \n",
    "    def __init__(self, input_dim, label_dim, hidden_dim):  \n",
    "        super(Discriminator, self).__init__()  \n",
    "        self.cnn = CNN(input_dim + label_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        x = self.cnn(x)\n",
    "        return x\n",
    "    \n",
    "# class Discriminator(nn.Module):  \n",
    "#     def __init__(self, input_dim, label_dim):  \n",
    "#         super(Discriminator, self).__init__()  \n",
    "#         self.fc1 = nn.Linear(input_dim + label_dim, 100) \n",
    "#         self.fc2 = nn.Linear(100,50)\n",
    "#         self.bn1 = nn.BatchNorm1d(100)   \n",
    "#         self.fc3 = nn.Linear(50, 50)  \n",
    "#         self.fc4 = nn.Linear(50, 1)                   \n",
    "#         self.bn2 = nn.BatchNorm1d(50)\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "#         self.sigmoid = nn.Sigmoid()  \n",
    "#         self.csa1 = ConditionalSelfAttention(100,label_dim,30)\n",
    "#         self.csa2 = ConditionalSelfAttention(50,label_dim,30)\n",
    "#         init.kaiming_normal_(self.fc1.weight, mode='fan_out', nonlinearity='relu')  \n",
    "#         init.kaiming_normal_(self.fc2.weight, mode='fan_out', nonlinearity='relu') \n",
    "#         init.kaiming_normal_(self.fc3.weight, mode='fan_out', nonlinearity='relu') \n",
    "#         init.kaiming_normal_(self.fc4.weight, mode='fan_out', nonlinearity='relu') \n",
    "  \n",
    "#     def forward(self, x, labels):\n",
    "#         # x = self.dropout(x) # 防止判别器性能过强\n",
    "#         x = self.fc1(x)  \n",
    "#         x = self.bn1(x)  \n",
    "#         x = F.leaky_relu(x)\n",
    "#         x = self.csa1(x,labels) # 判别器中运用条件自注意力\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = F.leaky_relu(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = F.leaky_relu(x)\n",
    "#         x = self.csa2(x,labels) # 判别器中运用条件自注意力\n",
    "#         x = self.fc4(x)\n",
    "#         x = self.sigmoid(x) # 输出一个概率值，是真数据还是假数据 表示输入数据是真实的概率\n",
    "#         return x  \n",
    "\n",
    "# 初始化生成器和判别器\n",
    "input_dim = 123  \n",
    "output_dim = 123\n",
    "label_dim = 5\n",
    "hidden_dim = 60\n",
    "generator = Generator(input_dim, label_dim, output_dim)\n",
    "discriminator = Discriminator(output_dim , label_dim, hidden_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa70571-571d-40bc-ad83-6c5527a56d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "Network Parameters： 79.249K\n",
      "FLOPs per sample： 79.742K\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "from thop import clever_format\n",
    "z = torch.randn(1,123)\n",
    "labels = torch.randn(1,5)\n",
    "flops_G, params_G = profile(generator, inputs=(z,labels))\n",
    "flops_G, params_G = clever_format([flops_G, params_G], '%.3f')\n",
    "\n",
    "print('Network Parameters：',params_G)\n",
    "print('FLOPs per sample：',flops_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ccbea03-666b-4168-b09a-bc908c76f9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool1d'>.\n",
      "Network Parameters： 42.384999999999998K\n",
      "FLOPs per sample： 1.060800000000000M\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,128)\n",
    "labels = torch.randn(1,5)\n",
    "flops_D, params_D = profile(discriminator, inputs=(x,labels))\n",
    "flops_D, params_D = clever_format([flops_D, params_D], '%.15f')\n",
    "\n",
    "print('Network Parameters：',params_D)\n",
    "print('FLOPs per sample：',flops_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859ca4cd-d218-45dc-96d8-90b106ee6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()  # 采用二元交叉熵损失函数\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0009)  # 异步更新\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.000007)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da4fcfa4-57c7-4b9b-8200-864a2707feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按指数调整学习率 每个epoch之后学习率乘0.7\n",
    "scheduler_D = torch.optim.lr_scheduler.ExponentialLR(optimizer_D, gamma=0.9999)\n",
    "scheduler_G = torch.optim.lr_scheduler.ExponentialLR(optimizer_D, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "041ac43a-1d72-4255-bc35-94594b0a4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 检查是否有GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 将模型移动到GPU上\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "# 将数据移动到GPU上\n",
    "X_train = X_train.to(device)\n",
    "y_train_encoded = y_train_encoded.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test_encoded = y_test_encoded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85dde5ce-c8d6-4b7a-85c4-0b707170e310",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generator:\n\tMissing key(s) in state_dict: \"fc21.weight\", \"fc21.bias\", \"fc22.weight\", \"fc22.bias\", \"fc23.weight\", \"fc23.bias\", \"fc24.weight\", \"fc24.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"bn4.weight\", \"bn4.bias\", \"bn4.running_mean\", \"bn4.running_var\", \"bn5.weight\", \"bn5.bias\", \"bn5.running_mean\", \"bn5.running_var\", \"csa1.gamma\", \"csa1.query.weight\", \"csa1.key.weight\", \"csa1.value.weight\", \"csa1.label_embedding.weight\", \"csa1.label_embedding.bias\", \"csa1.fc.weight\", \"csa1.fc.bias\", \"csa2.gamma\", \"csa2.query.weight\", \"csa2.key.weight\", \"csa2.value.weight\", \"csa2.label_embedding.weight\", \"csa2.label_embedding.bias\", \"csa2.fc.weight\", \"csa2.fc.bias\". \n\tUnexpected key(s) in state_dict: \"fc2.weight\", \"fc2.bias\", \"bn.weight\", \"bn.bias\", \"bn.running_mean\", \"bn.running_var\", \"bn.num_batches_tracked\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([200, 128]) from checkpoint, the shape in current model is torch.Size([100, 128]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([200]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([123, 200]) from checkpoint, the shape in current model is torch.Size([123, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_423728/1588083378.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloaded_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 加载模型参数到模型中\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# 打印加载成功的消息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CGAN model parameters have been loaded from {load_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1672\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tMissing key(s) in state_dict: \"fc21.weight\", \"fc21.bias\", \"fc22.weight\", \"fc22.bias\", \"fc23.weight\", \"fc23.bias\", \"fc24.weight\", \"fc24.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"bn4.weight\", \"bn4.bias\", \"bn4.running_mean\", \"bn4.running_var\", \"bn5.weight\", \"bn5.bias\", \"bn5.running_mean\", \"bn5.running_var\", \"csa1.gamma\", \"csa1.query.weight\", \"csa1.key.weight\", \"csa1.value.weight\", \"csa1.label_embedding.weight\", \"csa1.label_embedding.bias\", \"csa1.fc.weight\", \"csa1.fc.bias\", \"csa2.gamma\", \"csa2.query.weight\", \"csa2.key.weight\", \"csa2.value.weight\", \"csa2.label_embedding.weight\", \"csa2.label_embedding.bias\", \"csa2.fc.weight\", \"csa2.fc.bias\". \n\tUnexpected key(s) in state_dict: \"fc2.weight\", \"fc2.bias\", \"bn.weight\", \"bn.bias\", \"bn.running_mean\", \"bn.running_var\", \"bn.num_batches_tracked\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([200, 128]) from checkpoint, the shape in current model is torch.Size([100, 128]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([200]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([123, 200]) from checkpoint, the shape in current model is torch.Size([123, 100])."
     ]
    }
   ],
   "source": [
    "############加载模型\n",
    "# 选择一个加载模型参数的文件路径\n",
    "load_path ='cgan_model_para_bl_30_0007.pth'\n",
    "# 使用 torch.load() 函数加载模型参数\n",
    "loaded_parameters = torch.load(load_path)\n",
    "# 加载模型参数到模型中\n",
    "generator.load_state_dict(loaded_parameters)\n",
    "# 打印加载成功的消息  \n",
    "print(f\"CGAN model parameters have been loaded from {load_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b4a8ffb4-9641-4924-8c41-d6b545c6e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_losses_add = []\n",
    "g_losses_add = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d31f36e-2ae2-4140-af61-bf87145d0aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], D_Loss: 0.5331061317583833,G_Loss: 0.20381014069050626,fake_p:0.19800598555316562,real_p:0.8268166732055635\n",
      "Epoch [2/30], D_Loss: 0.2386542268462996,G_Loss: 0.07608891817372349,fake_p:0.07972360541725959,real_p:0.9339716977460306\n",
      "Epoch [3/30], D_Loss: 0.32901105760027816,G_Loss: 0.08739883401856621,fake_p:0.10276467856693838,real_p:0.9200447303707855\n",
      "Epoch [4/30], D_Loss: 0.3178268452561043,G_Loss: 0.13524963634873405,fake_p:0.09604304742212666,real_p:0.9260400684143837\n",
      "Epoch [5/30], D_Loss: 0.28078254649349427,G_Loss: 0.09660512652900542,fake_p:0.08473074800314952,real_p:0.9271057969406752\n",
      "Epoch [6/30], D_Loss: 0.3079653098530021,G_Loss: 0.10144987176574274,fake_p:0.0924614381359979,real_p:0.930163059141739\n",
      "Epoch [7/30], D_Loss: 0.3301951026919025,G_Loss: 0.12577203361761902,fake_p:0.09728975442985464,real_p:0.9179695555614465\n",
      "Epoch [8/30], D_Loss: 0.3902306240789479,G_Loss: 0.13526330538245324,fake_p:0.11363170180371193,real_p:0.9003751819004665\n",
      "Epoch [9/30], D_Loss: 0.4618186760838766,G_Loss: 0.12451656278846833,fake_p:0.13269430463348283,real_p:0.8867677183859322\n",
      "Epoch [10/30], D_Loss: 0.586809408241117,G_Loss: 0.11253453698860207,fake_p:0.17423758279056142,real_p:0.8462157368366866\n",
      "Epoch [11/30], D_Loss: 0.6534234008863694,G_Loss: 0.14223207888491263,fake_p:0.20492560960446085,real_p:0.832200913510227\n",
      "Epoch [12/30], D_Loss: 0.6165501213946116,G_Loss: 0.20132499901852027,fake_p:0.1837108396623406,real_p:0.8357426013783339\n",
      "Epoch [13/30], D_Loss: 0.5824365197477607,G_Loss: 0.33715401701818265,fake_p:0.1905313652042825,real_p:0.8393343069844339\n",
      "Epoch [14/30], D_Loss: 0.5941027447051809,G_Loss: 0.4030531949652028,fake_p:0.19485755369953467,real_p:0.8388121449730521\n",
      "Epoch [15/30], D_Loss: 0.5979744982850531,G_Loss: 0.4686434465162645,fake_p:0.18457214434392988,real_p:0.8317177797382455\n",
      "Epoch [16/30], D_Loss: 0.6773521459748095,G_Loss: 0.5081325561413305,fake_p:0.20818189276764554,real_p:0.8051255692119589\n",
      "Epoch [17/30], D_Loss: 0.8466470527870804,G_Loss: 0.6580457291929855,fake_p:0.2582230368329294,real_p:0.7767836802972358\n",
      "Epoch [18/30], D_Loss: 0.8872023822087322,G_Loss: 0.6136532873974234,fake_p:0.27837390480128704,real_p:0.7542416313237648\n",
      "Epoch [19/30], D_Loss: 1.0023884913945562,G_Loss: 0.741636892850629,fake_p:0.29275703340655157,real_p:0.7172955300961109\n",
      "Epoch [20/30], D_Loss: 0.9718272742987083,G_Loss: 0.8223250649302138,fake_p:0.27774041784552395,real_p:0.7260763079904228\n",
      "Epoch [21/30], D_Loss: 1.0437523954007428,G_Loss: 0.9705782792894974,fake_p:0.32530898198780056,real_p:0.7264037485326823\n",
      "Epoch [22/30], D_Loss: 0.9606205953559295,G_Loss: 0.72450453666261,fake_p:0.26755366853283213,real_p:0.7119610621762311\n",
      "Epoch [23/30], D_Loss: 1.102940405645548,G_Loss: 1.0873125462967732,fake_p:0.326416319969044,real_p:0.7059381403207331\n",
      "Epoch [24/30], D_Loss: 1.1247319459915162,G_Loss: 1.16248343347898,fake_p:0.34020804132626614,real_p:0.6797471974337553\n",
      "Epoch [25/30], D_Loss: 1.1491096794302693,G_Loss: 1.0534889362185134,fake_p:0.3307654311192861,real_p:0.6794559674753529\n",
      "Epoch [26/30], D_Loss: 1.1253663164508323,G_Loss: 0.9082647101528148,fake_p:0.3014414956044184,real_p:0.6854417266386015\n",
      "Epoch [27/30], D_Loss: 1.260352841044844,G_Loss: 1.1351989789662627,fake_p:0.3636697988805886,real_p:0.6481648657558007\n",
      "Epoch [28/30], D_Loss: 1.3350124811763087,G_Loss: 1.1514363871008007,fake_p:0.3930802097219451,real_p:0.641830824742739\n",
      "Epoch [29/30], D_Loss: 1.4050612215664988,G_Loss: 1.1709364351282265,fake_p:0.3907573435388489,real_p:0.6194966408343514\n",
      "Epoch [30/30], D_Loss: 1.374378557172927,G_Loss: 1.0336158567878801,fake_p:0.3670887073632887,real_p:0.6105978256395219\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "num_epochs = 30\n",
    "k = 3 # 先训练k轮判别器 再训练一轮生成器（交替训练）\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):  \n",
    "    discriminator_loss_sum = 0\n",
    "    generator_loss_sum = 0\n",
    "    fake_sum = 0\n",
    "    real_sum = 0\n",
    "    for j in range(k):\n",
    "        for i, (real_samples, real_labels) in enumerate(train_loader):  \n",
    "            #######训练判别器########\n",
    "            real_labels = real_labels.type(torch.FloatTensor).to(device) \n",
    "            optimizer_D.zero_grad()  \n",
    "            real_samples = real_samples.type(torch.FloatTensor).to(device)  \n",
    "            # 将真实的数据和条件（类别标签）输入到判别器中 判别器输出是真实样本的概率\n",
    "            real_output = discriminator(torch.cat((real_samples, real_labels), 1),real_labels) \n",
    "            real_sum += real_output[1].item()\n",
    "            # 判别器对所有真实样本的输出都接近1 torch.ones_like(real_output)表示一个与realoutput形状类型相同的1的张亮\n",
    "            real_loss = criterion(real_output, torch.ones_like(real_output))  \n",
    "\n",
    "            # 产生随机噪声：从标准正态分布中随机抽取值； 形成(real_samples.size(0)样本数量, input_dim噪声维度)形状\n",
    "            z = torch.randn(real_samples.size(0), input_dim).to(device)\n",
    "            # 噪声和条件标签生成假样本\n",
    "            fake_samples = generator(z, real_labels)\n",
    "            fake_output = discriminator(torch.cat((fake_samples, real_labels), 1),real_labels)\n",
    "            fake_sum += fake_output[1].item()\n",
    "            fake_loss = criterion(fake_output, torch.zeros_like(fake_output))\n",
    "            # 判别器总损失（两部分组成）\n",
    "            # # 给fakeloss加上权重\n",
    "            # p = 2\n",
    "            d_loss = real_loss + fake_loss\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            discriminator_loss_sum += d_loss.item()\n",
    "            # if(i+1)%200 == 0:\n",
    "            #     print(d_loss.item())\n",
    "    scheduler_D.step()\n",
    "        \n",
    "    for i, (real_samples, real_labels) in enumerate(train_loader):  \n",
    "        ########训练生成器########\n",
    "        real_samples = real_samples.to(device)\n",
    "        real_labels = real_labels.to(device)\n",
    "        optimizer_G.zero_grad() \n",
    "        # 生成假样本\n",
    "        z = torch.randn(real_samples.size(0), input_dim).to(device)\n",
    "        fake_samples = generator(z, real_labels)\n",
    "        # 判别器应该认为这些假样本是真实的\n",
    "        fake_output = discriminator(torch.cat((fake_samples, real_labels), 1),real_labels)\n",
    "        g_loss = criterion(fake_output, torch.ones_like(fake_output))\n",
    "        # 反向传播和优化生成器\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        generator_loss_sum += g_loss.item()\n",
    "    generator_avg_loss = generator_loss_sum / (len(train_loader))\n",
    "    # 计算判别器k轮的平均损失\n",
    "    discriminator_loss_avg = discriminator_loss_sum / (len(train_loader) * k)\n",
    "    g_losses.append(generator_avg_loss)\n",
    "    d_losses.append(discriminator_loss_avg)\n",
    "    fake_avg = fake_sum / (len(train_loader) * k)\n",
    "    real_avg = real_sum  / (len(train_loader) * k)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], D_Loss: {discriminator_loss_avg},G_Loss: {generator_avg_loss},fake_p:{fake_avg},real_p:{real_avg}\")\n",
    "    scheduler_G.step()\n",
    "    ##判别器的损失增大可能是因为生成器逐渐逼真，判别能力逐渐下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "475731a7-1dba-4bde-a8ad-3452f402ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_losses_add += d_losses\n",
    "# g_losses_add += g_losses\n",
    "d_losses_add = d_losses_add[:60]\n",
    "g_losses_add = g_losses_add[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "024a8a05-2d5e-4865-b8e9-4fde7ea42cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制D和G轮平均损失的变化图\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(d_losses_add) + 1), d_losses_add, label='Discriminator Loss')\n",
    "plt.plot(range(1, len(g_losses_add) + 1), g_losses_add, label='Generator Loss')\n",
    "plt.title('Losses over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7963a47-1b9a-4e84-9b20-08019127d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (real_samples, real_labels) in enumerate(train_loader): \n",
    "#         real_samples = real_samples.to(device)\n",
    "#         real_labels = real_labels.to(device)\n",
    "#         # 生成假样本\n",
    "#         z = torch.randn(real_samples.size(0), input_dim).to(device)\n",
    "#         fake_samples = generator(z, real_labels)\n",
    "#         fake_output = discriminator(torch.cat((fake_samples, real_labels), 1))\n",
    "#         real_labels = real_labels.type(torch.FloatTensor).to(device) \n",
    "#         real_samples = real_samples.type(torch.FloatTensor).to(device)  \n",
    "#         real_output = discriminator(torch.cat((real_samples, real_labels), 1))\n",
    "#         print(f\"real_output: {real_output[1]},fake_output: {fake_output[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a7a0a5b7-6b18-458d-ac77-b8f06c59f859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cgan model parameters have been saved to cgan_model_para_bl_60ADA.pth\n",
      "discrim model parameters have been saved to discrim_model_para_bl_60ADA.pth\n"
     ]
    }
   ],
   "source": [
    "# 获取模型参数  \n",
    "cgan_model_parameters = generator.state_dict()  \n",
    "# 选择一个保存模型参数的文件路径  \n",
    "save_path = 'cgan_model_para_bl_60ADA.pth'  \n",
    "# 使用 torch.save() 函数保存模型参数  \n",
    "torch.save(cgan_model_parameters, save_path)  \n",
    "# 打印保存成功的消息  \n",
    "print(f\"cgan model parameters have been saved to {save_path}\")\n",
    "\n",
    " \n",
    "discrim_model_parameters = generator.state_dict()  \n",
    "# 选择一个保存模型参数的文件路径  \n",
    "save_path = 'discrim_model_para_bl_60ADA.pth'  \n",
    "# 使用 torch.save() 函数保存模型参数  \n",
    "torch.save(discrim_model_parameters, save_path)  \n",
    "# 打印保存成功的消息  \n",
    "print(f\"discrim model parameters have been saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43363483-7b9f-4052-9ef3-77698a2203e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 123])\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Normal\n",
    "\n",
    "# 通过生成器可以指定类别生成新数据：与训练所用数据相似但不相同  \n",
    "def generate_new_data(generator,target_category, noise_dim, num_samples = 1):  \n",
    "    generator.eval()\n",
    "    # 准备目标类别的独热编码y  \n",
    "    # target_category = 2  #（索引从0开始） \n",
    "    num_classes = 5\n",
    "    y_onehot = torch.zeros(num_samples, num_classes)  # num_classes是类别的总数  \n",
    "    y_onehot[:, target_category] = 1  # 设置每一个样本目标类别的位置为1  \n",
    "    y_onehot = y_onehot.to(device)\n",
    "    # 从标准正态分布中随机采样噪声  \n",
    "    noise = torch.randn(num_samples, noise_dim).to(device)  \n",
    "    # 使用生成器生成数据  \n",
    "    with torch.no_grad():  \n",
    "        # 解码隐变量和标签以生成数据  \n",
    "        generated_data = generator(noise, y_onehot)  \n",
    "  \n",
    "    return generated_data\n",
    "\n",
    "# 设置要生成的数据数量\n",
    "num_samples = 10\n",
    "target_category = 0\n",
    "# 生成新数据\n",
    "new_data = generate_new_data(generator, target_category, 123, num_samples)\n",
    "\n",
    "# 打印生成的数据的形状\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fd4616fa-62b9-44f6-9353-c055211b3ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7985,  7.4932, 15.5198,  ..., -1.0452,  3.4480,  0.0000],\n",
      "        [ 1.7997,  7.5103, 15.5229,  ..., -1.0510,  3.4467,  0.0000],\n",
      "        [ 1.8137,  7.4756, 15.5375,  ..., -1.0360,  3.4478,  0.0000],\n",
      "        ...,\n",
      "        [ 4.2400,  3.2937, 39.5195,  ...,  3.8513, -0.2268,  0.0000],\n",
      "        [ 1.8043,  7.5077, 15.5371,  ..., -1.0493,  3.4468,  0.0000],\n",
      "        [ 1.7801,  7.5465, 15.5133,  ..., -1.0661,  3.4456,  0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 分别生成 DOS 0类型样本 21416 个，Probe 2类型样本55687 个\n",
    "# R2L 3类型样本 66348，U2R 4类型样本 67291 个\n",
    "# 生成的样本与原样本混合，形成 VAE 过采样后的训练集，使得每种类别样本的数量都为 67343个。\n",
    "# 设置要生成的数据数量  \n",
    "num_samples = 21416  \n",
    "target_category = 0\n",
    "# 生成新数据  \n",
    "new_data_dos = generate_new_data(generator, target_category, 123, num_samples)  \n",
    "\n",
    "#生成标签的一维Tensor\n",
    "labels = torch.full((num_samples,), target_category, dtype=torch.long)    \n",
    "labels = labels.view(num_samples, 1)  \n",
    "labels = labels.to(device)\n",
    "# 可以沿着最后一个维度拼接数据和标签  \n",
    "new_data_dos = torch.cat((new_data_dos, labels), dim=-1)  \n",
    "  \n",
    "# 输出结果以验证  \n",
    "print(new_data_dos)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01eba08-43aa-49a3-84be-dd5493391351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d75e39ff-60fc-4801-bff3-8c2411e5857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12.0640, 20.4706, -1.2039,  ...,  2.6116, -2.6637,  2.0000],\n",
      "        [12.1584, 20.5409, -1.2961,  ...,  2.6316, -2.6971,  2.0000],\n",
      "        [12.2531, 20.6231, -1.3299,  ...,  2.6365, -2.7353,  2.0000],\n",
      "        ...,\n",
      "        [ 0.6887, 18.7603, 13.2584,  ..., -3.5146,  1.3614,  2.0000],\n",
      "        [ 0.5209, 18.7096, 13.3173,  ..., -3.5893,  1.4620,  2.0000],\n",
      "        [12.1701, 20.5416, -1.2848,  ...,  2.6341, -2.7083,  2.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 设置要生成的数据数量  \n",
    "num_samples = 55687  \n",
    "target_category = 2\n",
    "# 生成新数据  \n",
    "new_data_probe = generate_new_data(generator, target_category, 123, num_samples)  \n",
    "\n",
    "#生成标签的一维Tensor\n",
    "labels = torch.full((num_samples,), target_category, dtype=torch.long)  \n",
    "# 为了拼接将标签Tensor扩展到与数据Tensor相同的维度  \n",
    "labels = labels.view(num_samples, 1)  \n",
    "labels = labels.to(device)\n",
    "# 沿着最后一个维度拼接数据和标签  \n",
    "new_data_probe = torch.cat((new_data_probe, labels), dim=-1)  \n",
    "  \n",
    "# 输出结果以验证  \n",
    "print(new_data_probe)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "98048ce4-3b53-49cc-8798-891a1bc2822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2691, 17.6626, 13.9976,  ..., -3.1791,  1.6086,  3.0000],\n",
      "        [11.5919, 19.6011, -0.8965,  ...,  2.5569, -2.6976,  3.0000],\n",
      "        [11.5677, 19.5801, -0.8871,  ...,  2.5482, -2.6885,  3.0000],\n",
      "        ...,\n",
      "        [11.4000, 19.4938, -0.7454,  ...,  2.4932, -2.5971,  3.0000],\n",
      "        [ 0.5963, 17.9817, 13.4952,  ..., -3.2579,  1.3528,  3.0000],\n",
      "        [11.5493, 19.6024, -0.8668,  ...,  2.5287, -2.6702,  3.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 设置要生成的数据数量  \n",
    "num_samples = 66348  \n",
    "target_category = 3\n",
    "# 生成新数据  \n",
    "new_data_r2l = generate_new_data(generator, target_category, 123, num_samples)  \n",
    "\n",
    "#生成标签的一维Tensor\n",
    "labels = torch.full((num_samples,), target_category, dtype=torch.long)  \n",
    "# 为了拼接将标签Tensor扩展到与数据Tensor相同的维度  \n",
    "labels = labels.view(num_samples, 1)  \n",
    "labels = labels.to(device)\n",
    "# 沿着最后一个维度拼接数据和标签  \n",
    "new_data_r2l = torch.cat((new_data_r2l, labels), dim=-1)  \n",
    "  \n",
    "# 输出结果以验证  \n",
    "print(new_data_r2l)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0517b412-6ac6-448a-95da-a0df7baf557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67291, 123])\n",
      "tensor([[ 0.7440, 17.9053, 14.5484,  ..., -3.1012,  1.1550,  4.0000],\n",
      "        [ 6.9748, 16.3556,  3.1306,  ...,  0.4822,  0.0788,  4.0000],\n",
      "        [ 6.9580, 16.3304,  3.1415,  ...,  0.4806,  0.0867,  4.0000],\n",
      "        ...,\n",
      "        [ 6.9884, 16.3060,  3.1794,  ...,  0.4870,  0.0774,  4.0000],\n",
      "        [ 6.9889, 16.3935,  3.1452,  ...,  0.4585,  0.0933,  4.0000],\n",
      "        [ 6.9603, 16.3111,  3.1703,  ...,  0.4829,  0.0904,  4.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 设置要生成的数据数量  \n",
    "num_samples = 67291  \n",
    "target_category = 4\n",
    "# 生成新数据  \n",
    "new_data_u2r = generate_new_data(generator, target_category, 123, num_samples)  \n",
    "print(new_data_u2r.shape)\n",
    "\n",
    "#生成标签的一维Tensor\n",
    "labels = torch.full((num_samples,), target_category, dtype=torch.long)  \n",
    "# 为了拼接将标签Tensor扩展到与数据Tensor相同的维度  \n",
    "labels = labels.view(num_samples, 1)  \n",
    "labels = labels.to(device)\n",
    "# 沿着最后一个维度拼接数据和标签  \n",
    "new_data_u2r = torch.cat((new_data_u2r, labels), dim=-1)  \n",
    "  \n",
    "# 输出结果以验证  \n",
    "print(new_data_u2r)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d1f46f31-d798-4167-9cee-47a3d3e9b242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 沿着第0维（通常是批次维度）拼接数据  \n",
    "combined_data = torch.cat((new_data_u2r, new_data_r2l,new_data_probe,new_data_dos), dim=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "63145106-7f4d-4ff0-87c7-16757dd4effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.cpu()\n",
    "combined_data = combined_data.numpy()\n",
    "# print(new_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "98e2c486-281a-4a62-8d55-240e041db0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.DataFrame(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "944a32fd-df5a-42fd-b808-278e3f7376e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./数据/KDDTrain+afterP.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ad3d8d55-46b8-4580-8ec9-580ec39b605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
      "       'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
      "       ...\n",
      "       'flag_REJ', 'flag_RSTO', 'flag_RSTOS0', 'flag_RSTR', 'flag_S0',\n",
      "       'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF', 'flag_SH'],\n",
      "      dtype='object', length=124)\n"
     ]
    }
   ],
   "source": [
    "# 添加特征属性表头\n",
    "columns = train_df.columns\n",
    "print(columns)\n",
    "columns = columns.drop('attack_type')\n",
    "columns = columns.append(pd.Index(['attack_type']))\n",
    "combined_data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6f421f99-6170-464e-857e-f4b0ea3b31a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_RSTO</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "      <th>attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.743990</td>\n",
       "      <td>17.905312</td>\n",
       "      <td>14.548405</td>\n",
       "      <td>-0.390266</td>\n",
       "      <td>2.553873</td>\n",
       "      <td>-1.031263</td>\n",
       "      <td>-2.654072</td>\n",
       "      <td>0.568918</td>\n",
       "      <td>-0.744580</td>\n",
       "      <td>2.867695</td>\n",
       "      <td>...</td>\n",
       "      <td>1.833644</td>\n",
       "      <td>0.955187</td>\n",
       "      <td>-0.143352</td>\n",
       "      <td>1.424710</td>\n",
       "      <td>2.434413</td>\n",
       "      <td>-1.973229</td>\n",
       "      <td>1.576041</td>\n",
       "      <td>-3.101226</td>\n",
       "      <td>1.155040</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.974848</td>\n",
       "      <td>16.355623</td>\n",
       "      <td>3.130611</td>\n",
       "      <td>-1.123970</td>\n",
       "      <td>-2.475618</td>\n",
       "      <td>1.098424</td>\n",
       "      <td>-2.864342</td>\n",
       "      <td>-2.142630</td>\n",
       "      <td>-0.717429</td>\n",
       "      <td>2.050294</td>\n",
       "      <td>...</td>\n",
       "      <td>1.807561</td>\n",
       "      <td>3.046221</td>\n",
       "      <td>2.661375</td>\n",
       "      <td>-1.519997</td>\n",
       "      <td>3.171882</td>\n",
       "      <td>-5.007968</td>\n",
       "      <td>-2.570246</td>\n",
       "      <td>0.482153</td>\n",
       "      <td>0.078770</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.958029</td>\n",
       "      <td>16.330437</td>\n",
       "      <td>3.141473</td>\n",
       "      <td>-1.129525</td>\n",
       "      <td>-2.468615</td>\n",
       "      <td>1.095661</td>\n",
       "      <td>-2.857370</td>\n",
       "      <td>-2.135950</td>\n",
       "      <td>-0.722228</td>\n",
       "      <td>2.045912</td>\n",
       "      <td>...</td>\n",
       "      <td>1.806171</td>\n",
       "      <td>3.043799</td>\n",
       "      <td>2.658151</td>\n",
       "      <td>-1.516592</td>\n",
       "      <td>3.164172</td>\n",
       "      <td>-4.997855</td>\n",
       "      <td>-2.563186</td>\n",
       "      <td>0.480574</td>\n",
       "      <td>0.086684</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.960981</td>\n",
       "      <td>16.333990</td>\n",
       "      <td>3.194793</td>\n",
       "      <td>-1.152159</td>\n",
       "      <td>-2.435487</td>\n",
       "      <td>1.079725</td>\n",
       "      <td>-2.858330</td>\n",
       "      <td>-2.111669</td>\n",
       "      <td>-0.740223</td>\n",
       "      <td>2.028940</td>\n",
       "      <td>...</td>\n",
       "      <td>1.808921</td>\n",
       "      <td>3.048339</td>\n",
       "      <td>2.675719</td>\n",
       "      <td>-1.500343</td>\n",
       "      <td>3.156682</td>\n",
       "      <td>-4.991720</td>\n",
       "      <td>-2.551139</td>\n",
       "      <td>0.450510</td>\n",
       "      <td>0.112386</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.994335</td>\n",
       "      <td>16.407789</td>\n",
       "      <td>3.126374</td>\n",
       "      <td>-1.126237</td>\n",
       "      <td>-2.467389</td>\n",
       "      <td>1.095887</td>\n",
       "      <td>-2.873309</td>\n",
       "      <td>-2.136044</td>\n",
       "      <td>-0.715751</td>\n",
       "      <td>2.055977</td>\n",
       "      <td>...</td>\n",
       "      <td>1.814848</td>\n",
       "      <td>3.056046</td>\n",
       "      <td>2.671562</td>\n",
       "      <td>-1.514552</td>\n",
       "      <td>3.182873</td>\n",
       "      <td>-5.021682</td>\n",
       "      <td>-2.572280</td>\n",
       "      <td>0.466146</td>\n",
       "      <td>0.083564</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210737</th>\n",
       "      <td>4.190221</td>\n",
       "      <td>3.793887</td>\n",
       "      <td>38.166725</td>\n",
       "      <td>2.758650</td>\n",
       "      <td>4.911016</td>\n",
       "      <td>-4.019784</td>\n",
       "      <td>1.082382</td>\n",
       "      <td>5.691416</td>\n",
       "      <td>2.234522</td>\n",
       "      <td>1.402677</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.457898</td>\n",
       "      <td>2.783920</td>\n",
       "      <td>2.332080</td>\n",
       "      <td>0.586663</td>\n",
       "      <td>2.774736</td>\n",
       "      <td>-1.771665</td>\n",
       "      <td>-0.614561</td>\n",
       "      <td>3.596234</td>\n",
       "      <td>-0.043423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210738</th>\n",
       "      <td>1.800973</td>\n",
       "      <td>7.513349</td>\n",
       "      <td>15.530334</td>\n",
       "      <td>-0.271497</td>\n",
       "      <td>3.496268</td>\n",
       "      <td>-0.927710</td>\n",
       "      <td>-1.396332</td>\n",
       "      <td>-0.926335</td>\n",
       "      <td>-2.462730</td>\n",
       "      <td>-2.458428</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.727349</td>\n",
       "      <td>-3.035388</td>\n",
       "      <td>-1.175158</td>\n",
       "      <td>-0.027616</td>\n",
       "      <td>-1.404333</td>\n",
       "      <td>1.698573</td>\n",
       "      <td>1.701501</td>\n",
       "      <td>-1.052274</td>\n",
       "      <td>3.447053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210739</th>\n",
       "      <td>4.239963</td>\n",
       "      <td>3.293680</td>\n",
       "      <td>39.519531</td>\n",
       "      <td>3.004171</td>\n",
       "      <td>5.005273</td>\n",
       "      <td>-4.100204</td>\n",
       "      <td>1.053031</td>\n",
       "      <td>6.186292</td>\n",
       "      <td>2.353124</td>\n",
       "      <td>1.233562</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.476358</td>\n",
       "      <td>2.714137</td>\n",
       "      <td>2.464343</td>\n",
       "      <td>0.499190</td>\n",
       "      <td>2.902958</td>\n",
       "      <td>-1.783833</td>\n",
       "      <td>-0.843347</td>\n",
       "      <td>3.851295</td>\n",
       "      <td>-0.226835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210740</th>\n",
       "      <td>1.804272</td>\n",
       "      <td>7.507669</td>\n",
       "      <td>15.537122</td>\n",
       "      <td>-0.268398</td>\n",
       "      <td>3.498693</td>\n",
       "      <td>-0.926880</td>\n",
       "      <td>-1.399090</td>\n",
       "      <td>-0.926017</td>\n",
       "      <td>-2.461343</td>\n",
       "      <td>-2.463737</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.731458</td>\n",
       "      <td>-3.039913</td>\n",
       "      <td>-1.176316</td>\n",
       "      <td>-0.027678</td>\n",
       "      <td>-1.407268</td>\n",
       "      <td>1.703523</td>\n",
       "      <td>1.700087</td>\n",
       "      <td>-1.049299</td>\n",
       "      <td>3.446831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210741</th>\n",
       "      <td>1.780081</td>\n",
       "      <td>7.546493</td>\n",
       "      <td>15.513299</td>\n",
       "      <td>-0.270170</td>\n",
       "      <td>3.483769</td>\n",
       "      <td>-0.936257</td>\n",
       "      <td>-1.378781</td>\n",
       "      <td>-0.921376</td>\n",
       "      <td>-2.466086</td>\n",
       "      <td>-2.442695</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.711531</td>\n",
       "      <td>-3.018598</td>\n",
       "      <td>-1.167943</td>\n",
       "      <td>-0.031641</td>\n",
       "      <td>-1.399255</td>\n",
       "      <td>1.677959</td>\n",
       "      <td>1.705169</td>\n",
       "      <td>-1.066051</td>\n",
       "      <td>3.445624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210742 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration  src_bytes  dst_bytes      land  wrong_fragment    urgent  \\\n",
       "0       0.743990  17.905312  14.548405 -0.390266        2.553873 -1.031263   \n",
       "1       6.974848  16.355623   3.130611 -1.123970       -2.475618  1.098424   \n",
       "2       6.958029  16.330437   3.141473 -1.129525       -2.468615  1.095661   \n",
       "3       6.960981  16.333990   3.194793 -1.152159       -2.435487  1.079725   \n",
       "4       6.994335  16.407789   3.126374 -1.126237       -2.467389  1.095887   \n",
       "...          ...        ...        ...       ...             ...       ...   \n",
       "210737  4.190221   3.793887  38.166725  2.758650        4.911016 -4.019784   \n",
       "210738  1.800973   7.513349  15.530334 -0.271497        3.496268 -0.927710   \n",
       "210739  4.239963   3.293680  39.519531  3.004171        5.005273 -4.100204   \n",
       "210740  1.804272   7.507669  15.537122 -0.268398        3.498693 -0.926880   \n",
       "210741  1.780081   7.546493  15.513299 -0.270170        3.483769 -0.936257   \n",
       "\n",
       "             hot  num_failed_logins  logged_in  num_compromised  ...  \\\n",
       "0      -2.654072           0.568918  -0.744580         2.867695  ...   \n",
       "1      -2.864342          -2.142630  -0.717429         2.050294  ...   \n",
       "2      -2.857370          -2.135950  -0.722228         2.045912  ...   \n",
       "3      -2.858330          -2.111669  -0.740223         2.028940  ...   \n",
       "4      -2.873309          -2.136044  -0.715751         2.055977  ...   \n",
       "...          ...                ...        ...              ...  ...   \n",
       "210737  1.082382           5.691416   2.234522         1.402677  ...   \n",
       "210738 -1.396332          -0.926335  -2.462730        -2.458428  ...   \n",
       "210739  1.053031           6.186292   2.353124         1.233562  ...   \n",
       "210740 -1.399090          -0.926017  -2.461343        -2.463737  ...   \n",
       "210741 -1.378781          -0.921376  -2.466086        -2.442695  ...   \n",
       "\n",
       "        flag_RSTO  flag_RSTOS0  flag_RSTR   flag_S0   flag_S1   flag_S2  \\\n",
       "0        1.833644     0.955187  -0.143352  1.424710  2.434413 -1.973229   \n",
       "1        1.807561     3.046221   2.661375 -1.519997  3.171882 -5.007968   \n",
       "2        1.806171     3.043799   2.658151 -1.516592  3.164172 -4.997855   \n",
       "3        1.808921     3.048339   2.675719 -1.500343  3.156682 -4.991720   \n",
       "4        1.814848     3.056046   2.671562 -1.514552  3.182873 -5.021682   \n",
       "...           ...          ...        ...       ...       ...       ...   \n",
       "210737  -2.457898     2.783920   2.332080  0.586663  2.774736 -1.771665   \n",
       "210738  -1.727349    -3.035388  -1.175158 -0.027616 -1.404333  1.698573   \n",
       "210739  -2.476358     2.714137   2.464343  0.499190  2.902958 -1.783833   \n",
       "210740  -1.731458    -3.039913  -1.176316 -0.027678 -1.407268  1.703523   \n",
       "210741  -1.711531    -3.018598  -1.167943 -0.031641 -1.399255  1.677959   \n",
       "\n",
       "         flag_S3   flag_SF   flag_SH  attack_type  \n",
       "0       1.576041 -3.101226  1.155040            4  \n",
       "1      -2.570246  0.482153  0.078770            4  \n",
       "2      -2.563186  0.480574  0.086684            4  \n",
       "3      -2.551139  0.450510  0.112386            4  \n",
       "4      -2.572280  0.466146  0.083564            4  \n",
       "...          ...       ...       ...          ...  \n",
       "210737 -0.614561  3.596234 -0.043423            0  \n",
       "210738  1.701501 -1.052274  3.447053            0  \n",
       "210739 -0.843347  3.851295 -0.226835            0  \n",
       "210740  1.700087 -1.049299  3.446831            0  \n",
       "210741  1.705169 -1.066051  3.445624            0  \n",
       "\n",
       "[210742 rows x 124 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将最后一列的float类型转换为整数类型  \n",
    "combined_data['attack_type'] = combined_data['attack_type'].astype(int)  \n",
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "602b48b3-1b45-4289-8443-1f4046e350aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把attack_type的操作应用在原始数据集中\n",
    "train_df = train_df[columns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "47ceba5c-75d7-4c3d-a1ff-847a10c90f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_column = 'attack_type'\n",
    "  \n",
    "# 定义字符串标签到整数的映射  \n",
    "label_mapping = {'dos': 0, 'normal': 1, 'probe':2, 'r2l':3, 'u2r':4}  \n",
    "\n",
    "train_df[last_column] = train_df[last_column].replace(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c70f97c-02df-4017-9836-714b767eef0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_RSTO</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "      <th>attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125968</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125969</th>\n",
       "      <td>8</td>\n",
       "      <td>105</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125970</th>\n",
       "      <td>0</td>\n",
       "      <td>2231</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125971</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125972</th>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125973 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration  src_bytes  dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
       "0              0        491          0     0               0       0    0   \n",
       "1              0        146          0     0               0       0    0   \n",
       "2              0          0          0     0               0       0    0   \n",
       "3              0        232       8153     0               0       0    0   \n",
       "4              0        199        420     0               0       0    0   \n",
       "...          ...        ...        ...   ...             ...     ...  ...   \n",
       "125968         0          0          0     0               0       0    0   \n",
       "125969         8        105        145     0               0       0    0   \n",
       "125970         0       2231        384     0               0       0    0   \n",
       "125971         0          0          0     0               0       0    0   \n",
       "125972         0        151          0     0               0       0    0   \n",
       "\n",
       "        num_failed_logins  logged_in  num_compromised  ...  flag_RSTO  \\\n",
       "0                       0          0                0  ...          0   \n",
       "1                       0          0                0  ...          0   \n",
       "2                       0          0                0  ...          0   \n",
       "3                       0          1                0  ...          0   \n",
       "4                       0          1                0  ...          0   \n",
       "...                   ...        ...              ...  ...        ...   \n",
       "125968                  0          0                0  ...          0   \n",
       "125969                  0          0                0  ...          0   \n",
       "125970                  0          1                0  ...          0   \n",
       "125971                  0          0                0  ...          0   \n",
       "125972                  0          1                0  ...          0   \n",
       "\n",
       "        flag_RSTOS0  flag_RSTR  flag_S0  flag_S1  flag_S2  flag_S3  flag_SF  \\\n",
       "0                 0          0        0        0        0        0        1   \n",
       "1                 0          0        0        0        0        0        1   \n",
       "2                 0          0        1        0        0        0        0   \n",
       "3                 0          0        0        0        0        0        1   \n",
       "4                 0          0        0        0        0        0        1   \n",
       "...             ...        ...      ...      ...      ...      ...      ...   \n",
       "125968            0          0        1        0        0        0        0   \n",
       "125969            0          0        0        0        0        0        1   \n",
       "125970            0          0        0        0        0        0        1   \n",
       "125971            0          0        1        0        0        0        0   \n",
       "125972            0          0        0        0        0        0        1   \n",
       "\n",
       "        flag_SH  attack_type  \n",
       "0             0            1  \n",
       "1             0            1  \n",
       "2             0            0  \n",
       "3             0            1  \n",
       "4             0            1  \n",
       "...         ...          ...  \n",
       "125968        0            0  \n",
       "125969        0            1  \n",
       "125970        0            1  \n",
       "125971        0            0  \n",
       "125972        0            1  \n",
       "\n",
       "[125973 rows x 124 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "875c3011-04dd-44a4-b375-35f7c73c45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用concat函数上下拼接  \n",
    "result = pd.concat([train_df, combined_data])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a8384a0-f7da-4d44-94c3-f329a820f380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_RSTO</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "      <th>attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>491.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>8153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210737</th>\n",
       "      <td>4.190221</td>\n",
       "      <td>3.793887</td>\n",
       "      <td>38.166725</td>\n",
       "      <td>2.758650</td>\n",
       "      <td>4.911016</td>\n",
       "      <td>-4.019784</td>\n",
       "      <td>1.082382</td>\n",
       "      <td>5.691416</td>\n",
       "      <td>2.234522</td>\n",
       "      <td>1.402677</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.457898</td>\n",
       "      <td>2.783920</td>\n",
       "      <td>2.332080</td>\n",
       "      <td>0.586663</td>\n",
       "      <td>2.774736</td>\n",
       "      <td>-1.771665</td>\n",
       "      <td>-0.614561</td>\n",
       "      <td>3.596234</td>\n",
       "      <td>-0.043423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210738</th>\n",
       "      <td>1.800973</td>\n",
       "      <td>7.513349</td>\n",
       "      <td>15.530334</td>\n",
       "      <td>-0.271497</td>\n",
       "      <td>3.496268</td>\n",
       "      <td>-0.927710</td>\n",
       "      <td>-1.396332</td>\n",
       "      <td>-0.926335</td>\n",
       "      <td>-2.462730</td>\n",
       "      <td>-2.458428</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.727349</td>\n",
       "      <td>-3.035388</td>\n",
       "      <td>-1.175158</td>\n",
       "      <td>-0.027616</td>\n",
       "      <td>-1.404333</td>\n",
       "      <td>1.698573</td>\n",
       "      <td>1.701501</td>\n",
       "      <td>-1.052274</td>\n",
       "      <td>3.447053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210739</th>\n",
       "      <td>4.239963</td>\n",
       "      <td>3.293680</td>\n",
       "      <td>39.519531</td>\n",
       "      <td>3.004171</td>\n",
       "      <td>5.005273</td>\n",
       "      <td>-4.100204</td>\n",
       "      <td>1.053031</td>\n",
       "      <td>6.186292</td>\n",
       "      <td>2.353124</td>\n",
       "      <td>1.233562</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.476358</td>\n",
       "      <td>2.714137</td>\n",
       "      <td>2.464343</td>\n",
       "      <td>0.499190</td>\n",
       "      <td>2.902958</td>\n",
       "      <td>-1.783833</td>\n",
       "      <td>-0.843347</td>\n",
       "      <td>3.851295</td>\n",
       "      <td>-0.226835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210740</th>\n",
       "      <td>1.804272</td>\n",
       "      <td>7.507669</td>\n",
       "      <td>15.537122</td>\n",
       "      <td>-0.268398</td>\n",
       "      <td>3.498693</td>\n",
       "      <td>-0.926880</td>\n",
       "      <td>-1.399090</td>\n",
       "      <td>-0.926017</td>\n",
       "      <td>-2.461343</td>\n",
       "      <td>-2.463737</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.731458</td>\n",
       "      <td>-3.039913</td>\n",
       "      <td>-1.176316</td>\n",
       "      <td>-0.027678</td>\n",
       "      <td>-1.407268</td>\n",
       "      <td>1.703523</td>\n",
       "      <td>1.700087</td>\n",
       "      <td>-1.049299</td>\n",
       "      <td>3.446831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210741</th>\n",
       "      <td>1.780081</td>\n",
       "      <td>7.546493</td>\n",
       "      <td>15.513299</td>\n",
       "      <td>-0.270170</td>\n",
       "      <td>3.483769</td>\n",
       "      <td>-0.936257</td>\n",
       "      <td>-1.378781</td>\n",
       "      <td>-0.921376</td>\n",
       "      <td>-2.466086</td>\n",
       "      <td>-2.442695</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.711531</td>\n",
       "      <td>-3.018598</td>\n",
       "      <td>-1.167943</td>\n",
       "      <td>-0.031641</td>\n",
       "      <td>-1.399255</td>\n",
       "      <td>1.677959</td>\n",
       "      <td>1.705169</td>\n",
       "      <td>-1.066051</td>\n",
       "      <td>3.445624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336715 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration   src_bytes    dst_bytes      land  wrong_fragment    urgent  \\\n",
       "0       0.000000  491.000000     0.000000  0.000000        0.000000  0.000000   \n",
       "1       0.000000  146.000000     0.000000  0.000000        0.000000  0.000000   \n",
       "2       0.000000    0.000000     0.000000  0.000000        0.000000  0.000000   \n",
       "3       0.000000  232.000000  8153.000000  0.000000        0.000000  0.000000   \n",
       "4       0.000000  199.000000   420.000000  0.000000        0.000000  0.000000   \n",
       "...          ...         ...          ...       ...             ...       ...   \n",
       "210737  4.190221    3.793887    38.166725  2.758650        4.911016 -4.019784   \n",
       "210738  1.800973    7.513349    15.530334 -0.271497        3.496268 -0.927710   \n",
       "210739  4.239963    3.293680    39.519531  3.004171        5.005273 -4.100204   \n",
       "210740  1.804272    7.507669    15.537122 -0.268398        3.498693 -0.926880   \n",
       "210741  1.780081    7.546493    15.513299 -0.270170        3.483769 -0.936257   \n",
       "\n",
       "             hot  num_failed_logins  logged_in  num_compromised  ...  \\\n",
       "0       0.000000           0.000000   0.000000         0.000000  ...   \n",
       "1       0.000000           0.000000   0.000000         0.000000  ...   \n",
       "2       0.000000           0.000000   0.000000         0.000000  ...   \n",
       "3       0.000000           0.000000   1.000000         0.000000  ...   \n",
       "4       0.000000           0.000000   1.000000         0.000000  ...   \n",
       "...          ...                ...        ...              ...  ...   \n",
       "210737  1.082382           5.691416   2.234522         1.402677  ...   \n",
       "210738 -1.396332          -0.926335  -2.462730        -2.458428  ...   \n",
       "210739  1.053031           6.186292   2.353124         1.233562  ...   \n",
       "210740 -1.399090          -0.926017  -2.461343        -2.463737  ...   \n",
       "210741 -1.378781          -0.921376  -2.466086        -2.442695  ...   \n",
       "\n",
       "        flag_RSTO  flag_RSTOS0  flag_RSTR   flag_S0   flag_S1   flag_S2  \\\n",
       "0        0.000000     0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "1        0.000000     0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "2        0.000000     0.000000   0.000000  1.000000  0.000000  0.000000   \n",
       "3        0.000000     0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "4        0.000000     0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "...           ...          ...        ...       ...       ...       ...   \n",
       "210737  -2.457898     2.783920   2.332080  0.586663  2.774736 -1.771665   \n",
       "210738  -1.727349    -3.035388  -1.175158 -0.027616 -1.404333  1.698573   \n",
       "210739  -2.476358     2.714137   2.464343  0.499190  2.902958 -1.783833   \n",
       "210740  -1.731458    -3.039913  -1.176316 -0.027678 -1.407268  1.703523   \n",
       "210741  -1.711531    -3.018598  -1.167943 -0.031641 -1.399255  1.677959   \n",
       "\n",
       "         flag_S3   flag_SF   flag_SH  attack_type  \n",
       "0       0.000000  1.000000  0.000000            1  \n",
       "1       0.000000  1.000000  0.000000            1  \n",
       "2       0.000000  0.000000  0.000000            0  \n",
       "3       0.000000  1.000000  0.000000            1  \n",
       "4       0.000000  1.000000  0.000000            1  \n",
       "...          ...       ...       ...          ...  \n",
       "210737 -0.614561  3.596234 -0.043423            0  \n",
       "210738  1.701501 -1.052274  3.447053            0  \n",
       "210739 -0.843347  3.851295 -0.226835            0  \n",
       "210740  1.700087 -1.049299  3.446831            0  \n",
       "210741  1.705169 -1.066051  3.445624            0  \n",
       "\n",
       "[336715 rows x 124 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result #这就是用CVAE数据平衡化处理得到的最终结果，实现各个类别平衡，每个类别有67343个样本，总共有336715个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ec9b78fd-fa3c-415b-be61-2d99dae03eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存新的数据集\n",
    "result.to_csv('KDDTrain_CGAN_DCNN_selfatten_30test3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4335d7-bf06-4fd7-9668-95dc038ad74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f03fea-4a87-466c-a6e6-de9810e2de52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
